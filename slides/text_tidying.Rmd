---
title: "Tidy text and sentiment analysis"
subtitle: "JHU Data Science"
output:
  ioslides_presentation:
    css: ../styles.css
    widescreen: yes
---



```{r, echo = FALSE, message = FALSE}
library(knitr)
opts_chunk$set(comment = "", cache = TRUE)
library(readr)
library(stringr)
library(tidyr)
library(emo) # hadley/emo
library(ggplot2)
library(tm)
source("../slide_functions.R")
suppressPackageStartupMessages(library(dplyr))
```

## Data Cleaning

In general, data cleaning is a process of investigating your data for inaccuracies, or recoding it in a way that makes it more manageable.

`r emo::ji("warning")` MOST IMPORTANT RULE - LOOK `r emo::ji("eyes")` AT YOUR DATA! `r emo::ji("warning")`

# String functions

## Pasting strings with `paste` and `paste0`

Paste can be very useful for joining vectors together:

```{r Paste}
paste("Visit", 1:5, sep = "_")
paste("Visit", 1:5, sep = "_", collapse = " ")
paste("To", "is going be the ", "we go to the store!", sep = "day ")
# and paste0 can be even simpler see ?paste0 
paste0("Visit",1:5)
```

## Paste Depicting How Collapse Works

```{r Paste2}
paste(1:5)
paste(1:5, collapse = " ")
```

## Useful String Functions

Useful String functions

* `toupper()`, `tolower()` - uppercase or lowercase your data:
* `str_trim()` (in the `stringr` package) or `trimws` in base 
- will trim whitespace on ends
* `stringr::str_squish` - trims and replaces double spaces
* `nchar` - get the number of characters in a string


## The `stringr` package

Like `dplyr`, the `stringr` package:

* Makes some things more intuitive
* Is different than base R
* Is used on forums for answers
* Has a standard format for most functions
* the first argument is a string like first argument is a `data.frame` in `dplyr`


## 'Find' functions: `stringr`

`str_detect`, `str_subset`, `str_replace`, and `str_replace_all` search for matches to argument pattern within each element of a character vector: they differ in the format of and amount of detail in the results. 

* `str_detect` - returns `TRUE` if `pattern` is found
* `str_subset` - returns only the strings which pattern were detected
* convenient wrapper around `x[str_detect(x, pattern)]`
* `str_extract` - returns only strings which pattern were detected, but ONLY the pattern
* `str_replace` - replaces `pattern` with `replacement` the first time
* `str_replace_all` - replaces `pattern` with `replacement` as many times matched

## Let's look at modifier for `stringr`

`?modifiers`

* `fixed` - match everything exactly
* `regexp` - default - uses **reg**ular **exp**ressions
* `ignore_case` is an option to not have to use `tolower`
* `boundary` - Match boundaries between things (e.g. words, sentences, characters).



## Substringing

Very similar:

Base R

* `substr(x, start, stop)` - substrings from position start to position stop
* `strsplit(x, split)` - splits strings up - returns list!

`stringr`

* `str_sub(x, start, end)` - substrings from position start to position end
* `str_split(string, pattern)` - splits strings up - returns list!


## Splitting String: base R

In base R, `strsplit` splits a vector on a string into a `list`

```{r strsplit}
x = c("I really", "like writing", "R code programs")
(y = strsplit(x, split = " ")) # returns a list
(y2 = stringr::str_split(x, " ")) # returns a list
```

## Using a fixed expression

One example case is when you want to split on a period "`.`".  In regular expressions `.` means **ANY** character, so

```{r strsplitfixed}
str_split("I.like.strings", ".")
str_split("I.like.strings", fixed("."))
```

## Use `purrr` or `apply*` to extract from string lists

```{r stsplit2}
sapply(y, dplyr::first) # on the fly
purrr::map_chr(y, nth, 2) # on the fly
sapply(y, dplyr::last) # on the fly
```

## Boundary

We can use `boundary` in the case of `str_split` as well:

```{r}
words = c("These are   some words.")
str_count(words, boundary("word"))
# split with space
str_split(words, " ")[[1]]
# split between word
str_split(words, boundary("word"))[[1]]
```


## Splitting/Find/Replace and Regular Expressions

* R can do much more than find exact matches for a whole string
* Like Perl and other languages, it can use regular expressions.
* What are regular expressions?
* Ways to search for specific strings 
* Can be very complicated or simple
* Highly Useful - think "Find" on steroids


## A bit on Regular Expressions

* http://www.regular-expressions.info/reference.html
* They can use to match a large number of strings in one statement
* `.` matches any single character
* `*` means repeat as many (even if 0) more times the last character
* `?` makes the last thing optional
* `^` matches start of vector `^a` - starts with "a"
* `$` matches end of vector `b$` - ends with "b"

## Beginning of line with ^

```{r}
x = c("i think we all rule for participating",
      "i think i have been outed",
      "i think this will be quite fun actually",
      "it will be fun, i think")

str_detect(x, "^i think")
```


## End of line with $


```{r}
x = c("well they had something this morning",
      "then had to catch a tram home in the morning",
      "dog obedience school in the morning",
      "this morning I'll go for a run")

str_detect(x, "morning$")
```


## Character list with [ ]

```{r}
x = c("Name the worst thing about Bush!",
      "I saw a green bush",
      "BBQ and bushwalking at Molonglo Gorge",
      "BUSH!!")

str_detect(x,"[Bb][Uu][Ss][Hh]")
```


## Sets of letters and numbers

```{r}
x = c("7th inning stretch",
      "2nd half soon to begin. OSU did just win.",
      "3am - cant sleep - too hot still.. :(",
      "5ft 7 sent from heaven")

str_detect(x,"^[0-9][a-zA-Z]")
```

## Negative Classes

I want to match NOT a `?` or `.` at the end of line (fixed with `[ ]`).
```{r}
x = c("are you there?",
      "2nd half soon to begin. OSU did just win.",
      "6 and 9",
      "dont worry... we all die anyway!")

str_detect(x,"[^?.]$")
```


## . means anything

```{r}
x = c("these are post 9-11 rules",
      "NetBios: scanning ip 203.169.114.66",
      "Front Door 9:11:46 AM",
      "Sings: 0118999881999119725...3 !")

str_detect(x, "9.11")
```


## `|` means or

```{r}
x = c("Not a whole lot of hurricanes.",
      "We do have floods nearly every day", 
      "hurricanes swirl in the other direction",
      "coldfire is STRAIGHT!")

str_detect(x, "flood|earthquake|hurricane|coldfire")
```


## Detecting phone numbers

```{r}
x = c("206-555-1122","206-332","4545","test")

phone = "([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})"

str_detect(x,phone)
```



## Read in Salary Data
```{r}
suppressMessages({
  raw_salary_data = Sal = readr::read_csv(
    "https://raw.githubusercontent.com/muschellij2/adv_data_sci_2023/main/example_data/Baltimore_City_Employee_Salaries_FY2014.csv", 
    progress = FALSE)
})
head(Sal)
```

## 'Find' functions: finding values, `stringr` and `dplyr` 

```{r ggrep}
str_subset(Sal$Name, "Rawlings")
Sal %>% filter(str_detect(Name, "Rawlings"))
```

## Replacing and subbing: `stringr` 

We can do the same thing (with 2 piping operations!) in dplyr

```{r orderSal_stringr_rework}
dplyr_sal = Sal %>% mutate( 
  AnnualSalary = AnnualSalary %>%
    str_replace(fixed("$"), "") %>%
    as.numeric) %>%
  arrange(desc(AnnualSalary))
```

## Showing difference in `str_extract` and  `str_extract_all`

`str_extract_all` extracts all the matched strings
- `\\d` searches for DIGITS/numbers

```{r }
head(str_extract(Sal$AgencyID, "\\d"))
head(str_extract_all(Sal$AgencyID, "\\d"), 2)
```

## 'Find' functions: base R

`grep`: `grep`, `grepl`, `regexpr` and `gregexpr` search for matches to argument pattern within each element of a character vector: they differ in the format of and amount of detail in the results. 

`grep(pattern, x, fixed=FALSE)`, where:

* pattern = character string containing a regular expression to be matched in the given character vector.

* x = a character vector where matches are sought, or an object which can be coerced by as.character to a character vector.

* If fixed=TRUE, it will do exact matching for the phrase anywhere in the vector (regular find)


## 'Find' functions: stringr compared to base R

Base R does not use these functions.  Here is a "translator" of the `stringr` function to base R functions

* `str_detect` - similar to `grepl` (return logical)
* `grep(value = FALSE)` is similar to `which(str_detect())`
* `str_subset` - similar to `grep(value = TRUE)` - return value of matched
* `str_replace` - similar to `sub` - replace one time 
* `str_replace_all` - similar to `gsub` - replace many times

## Important Comparisons

Base R:

* Argument order is `(pattern, x)`
* Uses option `(fixed = TRUE)`

`stringr`

* Argument order is `(string, pattern)` aka `(x, pattern)`
* Uses function `fixed(pattern)`

## 'Find' functions: Finding Indices

These are the indices where the pattern match occurs:

```{r RawlMatch}
grep("Rawlings",Sal$Name)
which(grepl("Rawlings", Sal$Name))
which(str_detect(Sal$Name, "Rawlings"))
```

## 'Find' functions: Finding Logicals

These are the indices where the pattern match occurs:

```{r RawlMatch_log}
head(grepl("Rawlings",Sal$Name))
head(str_detect(Sal$Name, "Rawlings"))
```


## 'Find' functions: finding values, base R 

```{r grepl}
grep("Rawlings", Sal$Name, value=TRUE)
Sal[grep("Rawlings", Sal$Name), ]
```



## Showing differnce in `str_extract`

`str_extract` extracts just the matched string

```{r ggrep2}
ss = str_extract(Sal$Name, "Rawling")
head(ss)
ss[!is.na(ss)]
```

## Showing differnce in `str_extract` and  `str_extract_all`

`str_extract_all` extracts all the matched strings

```{r }
head(str_extract(Sal$AgencyID, "\\d"))
head(str_extract_all(Sal$AgencyID, "\\d"), 2)
```



## Using Regular Expressions
* Look for any name that starts with:
* Payne at the beginning, 
* Leonard and then an S
* Spence then capital C

```{r grepstar}
head(grep("^Payne.*", x = Sal$Name, value = TRUE), 3)
```

```{r grepstar2}
head(grep("Leonard.?S", x = Sal$Name, value = TRUE))
head(grep("Spence.*C.*", x = Sal$Name, value = TRUE))
```



## Using Regular Expressions: `stringr`
```{r grepstar_stringr}
head(str_subset(Sal$Name, "^Payne.*"), 3)
```

```{r grepstar2_stringr}
head(str_subset(Sal$Name, "Leonard.?S"))
head(str_subset(Sal$Name, "Spence.*C.*"))
```

## Replace

Let's say we wanted to sort the data set by Annual Salary:

```{r classSal}
class(Sal$AnnualSalary)
```

```{r orderstring}
sort(c("1", "2", "10")) #  not sort correctly (order simply ranks the data)
order(c("1", "2", "10"))
```

## Replace

So we must change the annual pay into a numeric:
```{r destringSal}
head(Sal$AnnualSalary, 4)
head(as.numeric(Sal$AnnualSalary), 4)
```

R didn't like the `$` so it thought turned them all to `NA`.

`sub()` and `gsub()` can do the replacing part in base R.

## Replacing and subbing 

Now we can replace the `$` with nothing (used `fixed=TRUE` because `$` means ending):

```{r orderSal}
Sal$AnnualSalary = as.numeric(gsub(pattern = "$", replacement="", 
                                   Sal$AnnualSalary, fixed=TRUE))
Sal = Sal %>% arrange(desc(AnnualSalary))
Sal %>% select(Name, AnnualSalary, JobTitle)
```

## Replacing and subbing: `stringr` 

We can do the same thing (with 2 piping operations!) in dplyr
```{r orderSal_stringr}
dplyr_sal = Sal %>% mutate( 
  AnnualSalary = AnnualSalary %>%
    str_replace(
      fixed("$"), 
      "") %>%
    as.numeric) %>%
  arrange(desc(AnnualSalary))
check_Sal = Sal
rownames(check_Sal) = NULL
all.equal(check_Sal, dplyr_sal) # they are the same
```

## Removing `$` and `,` in Practice

`readr::parse_*` is a number of useful helper functions for parsing columns

```{r parse_number}
head(readr::parse_number(raw_salary_data$AnnualSalary))
raw_salary_data %>% 
  mutate(across(matches("Salary|Pay"), readr::parse_number)) %>% 
  select(matches("Salary|Pay"))
```


## Back Referencing 

`( )` do grouping in regex, but you can also reference these groups with the the order in which they are in:

```{r backref}
x = c("Are you here?", "I think that is him.", "why didn't they?")
str_replace(
  x, 
  regex(".*(are|think|did).*([.?])", ignore_case = TRUE), 
  "The verb of the sentence was \\1 and the ending punctuation is \\2")
```
Note, you can't reference something that doesn't exist (no 3rd group):
```{r backref2, error=TRUE}
str_replace(x, 
            regex(".*(are|think|did).*([.?])", ignore_case = TRUE), 
            "verb: \\1, ending punctuation: \\2, and \\3")
```


```{r setup2, include = FALSE}
folder = "../imgs/tidytext/"
```

# Tidying Text - What about all of Jane Austen's Novels?

## Jane Austen

```{r ja, out.width="70%", echo = FALSE}
knitr::include_graphics("../imgs/tidytext/janeausten.png")
```

## Data Available via: `janeaustenr`

Attached with row numbers (by book).

```{r jane_austen_r}
library(janeaustenr)
original_books = austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number()) %>%
  ungroup()
head(original_books)
```


## TidyText 

```{r tidytext, out.width="70%", echo = FALSE}
knitr::include_graphics("../imgs/tidytext/tidytext.png")
```

http://joss.theoj.org/papers/89fd1099620268fe0342ffdcdf66776f


## A nice tutorial

```{r regex_tutorial, out.width="80%", echo = FALSE}
knitr::include_graphics("../imgs/tidytext/regex_tutorial.png")
```

http://stat545-ubc.github.io/block022_regular-expression.html



## Large workhorse function: `unnest_tokens`

```{r unnest_tokens}
library(tidytext)
txt = c("These are words", "so are these", "this is running on")
sentence = c(1, 2, 3)
dat = tibble(txt, sentence)
unnest_tokens(dat, tok, txt)
```


## What is tokenization? 

<div style='font-size:30pt'>

> "The process of segmenting running text into words and sentences."

- Split on white space/punctuation
- Make lower case 
- Keep contractions together
- Maybe put quoted words together (not in unnest_tokens)

</p>




## One token per row

```{r tidy_books}
tidy_books = original_books %>% unnest_tokens(word, text)
head(tidy_books)
```

## Stop words/words to filter

```{r wordcloud, out.width="70%", echo = FALSE}
knitr::include_graphics("../imgs/tidytext/wordcloud.png")
```

http://xpo6.com/list-of-english-stop-words/

## Wordcloud for Sense & Sensibility

```{r wordcloud_out}
tt = tidy_books %>% 
  filter(book == "Sense & Sensibility") %>% 
  count(word) %>% 
  arrange(desc(n)) %>% 
  slice(1:200L)
wordcloud::wordcloud(tt$word, tt$n)
```

## Stop words/words to filter

```{r}
tidy_books %>% 
  group_by(word) %>% 
  tally() %>% 
  arrange(desc(n))
```

## Top Words by Book

```{r}
tidy_books %>% 
  count(book, word) %>% 
  arrange(desc(n)) %>% 
  group_by(book) %>% 
  slice(1L)
```




## Filtering with joins

```{r}
head(stop_words)
(tidy_books = tidy_books %>% anti_join(stop_words, by = "word"))
```

## Top Words after joining

```{r}
tidy_books %>% 
  count(word) %>% 
  arrange(desc(n))
```
## Top Words by Book after joining

```{r}
top_book_words = tidy_books %>% 
  count(word, book) %>% 
  arrange(desc(n)) %>% 
  group_by(book) 
(top_book_words %>% slice(1:2))
```

## Wordcloud for Sense & Sensibility (no stopwords)

```{r wordcloud_nostop, warning=FALSE}
tt = tidy_books %>% 
  filter(book == "Sense & Sensibility") %>% 
  count(word) %>% 
  arrange(desc(n)) %>% 
  slice(1:200L)
wordcloud::wordcloud(tt$word, tt$n)
```




## Sentiment analysis

<font  style='font-size:40pt'>
"I hate this stupid class. But I love the work"
</font>



## Sentiment analysis

<font  style='font-size:40pt'>
"I <font color="red">hate</font> this <font color="red">stupid</font> class. But I <font color="blue">love</font> the work"
</font>


## Sentiment analysis

<font  style='font-size:40pt'>
"I <font color="red">hate</font> this <font color="red">stupid</font> class. But I <font color="blue">love</font> the work"
<br>
"Oh yeah, I totally <font color="blue">love</font> doing coding sessions"
</font>



## Sentiments

```{r bing}
bing = tidytext::sentiments
head(bing)
(dupes = bing %>% janitor::get_dupes(word))
```

## Sentiments: A little Tidying

Let's remove those cases that it says these duplicates were positive

```{r}
bing = bing %>% # remove positive envy!
  anti_join(dupes %>% filter(sentiment == "positive"))
anyDuplicated(bing$word) == 0
```

## Aside: `any(duplicated(x))` vs. `anyDuplicated(x) == 0` speed

Functions useful for checking and speed (big data): `anyNA` and `anyDuplicated
```{r}
microbenchmark::microbenchmark(
  anyDup = anyDuplicated(bing$word) == 0,
  any_dup = any(duplicated(bing$word)),
  anyNA = anyNA(bing$word),
  any_is_na = any(is.na(bing$word))
)
```

## Top Word Sentiments

Miss - may be misclassified (e.g. "Miss Elizabeth")
```{r}
top_book_words %>% slice(1:2) %>% left_join(bing, by = join_by(word))
```

## Top Word Sentiments

```{r}
top_book_words %>% inner_join(bing, by = join_by(word)) %>% slice(1:2)
```

## Assigning sentiments to words

```{r ja_tidy}
janeaustensentiment = tidy_books %>%
  inner_join(bing, by = join_by(word)) %>% 
  count(book, page = linenumber %/% 80, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)
head(janeaustensentiment)
```

## Assigning sentiments to words

```{r ja_tidy_book}
janeaustensentiment %>% 
  group_by(book) %>% 
  slice(1:3)
```

## Assigning sentiments to words: removing "miss"

In reality, you'd probably do more tidying on words like this, or a more sophisticated NLP approach (or `gsub("miss (elizabeth|elinor)", "\\1", x)`).
```{r ja_tidy_nomiss}
janeaustensentiment_nomiss = tidy_books %>%
  filter(word != "miss") %>% 
  inner_join(bing, by = join_by(word)) %>% 
  count(book, page = linenumber %/% 80, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)
```


## Plotting the sentiment trajectory (ggplot2 loaded)

```{r, fig.width = 10, fig.height = 5, out.width = "80%" }
ggplot(janeaustensentiment, aes(page, sentiment, fill = book)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~book, ncol = 3, scales = "free_x")
```


## Plotting the cumulative sentiment 

```{r cumsum, fig.width = 10, fig.height = 5, out.width = "80%" }
janeaustensentiment %>% 
  group_by(book) %>% 
  mutate(sentiment = cumsum(sentiment)) %>% 
  ggplot(aes(page, sentiment, colour = book)) +
  geom_line() + ylab("Cumulative Sentiment") + xlab("Pages(ish)")
```

## Plotting the cumulative sentiment (normalized book length)

```{r cumsum_norm, fig.width = 10, fig.height = 5, out.width = "80%" }
janeaustensentiment %>% 
  group_by(book) %>% 
  mutate(sentiment = cumsum(sentiment),
         page = page/max(page)) %>% 
  ggplot(aes(page, sentiment, colour = book)) +
  geom_line() + ylab("Cumulative Sentiment") + xlab("% Pages(ish)")
```


## Plotting the cumulative sentiment (normalized book length)

```{r cumsum_norm_nomiss, fig.width = 10, fig.height = 5, out.width = "80%" }
(p = janeaustensentiment_nomiss %>% 
   group_by(book) %>% 
   mutate(sentiment = cumsum(sentiment),
          page = page/max(page)) %>% 
   ggplot(aes(page, sentiment, colour = book)) + geom_line(linewidth = 1.25) +
   ylab("Cumulative Sentiment") + xlab("% Pages(ish)"))
```

## Plotting Aside: Put Legend **Inside** the Figure

```{r trans_leg, echo = FALSE, dependson="cumsum_norm_nomiss", fig.width = 10, fig.height = 5, out.width = "100%"}
transparent_legend =  theme(
  legend.background = element_rect(fill = "transparent"),
  legend.key = element_rect(fill = "transparent", 
                            color = "transparent"))
p + 
  transparent_legend + 
  scale_color_brewer(type = "qual") + 
  scale_x_continuous(labels = scales::percent_format()) +
  theme(legend.position = c(0.2, 0.7), text = element_text(size = 20)) + 
  guides(colour = guide_legend(title = "Book", 
                               override.aes = list(linewidth = 2)))
```

## Plotting Aside: Put Legend **Inside** the Figure

```{r trans_leg_show, eval = FALSE}
<<trans_leg>>
```



## Stemming

Can use `wordStem` to reduce certain words to their primary stem (e.g. remove gerunds/tense):

```{r}
library(SnowballC)
wordStem(c("running","fasted"))
```

## Stemming

Stemming gets you the root of the word, but the stem may be odd:

```{r}
tidy_books %>% 
  mutate(stem = SnowballC::wordStem(word)) %>% 
  filter(stem != word) %>% 
  head(20)
```





# Extra slides

## Other Text packages;

- Text Mining in R - [`tm`](https://cran.r-project.org/web/packages/tm/index.html)
- Latent Dirichlet Allocation (LDA) models and Correlated Topics Models (CTM) - [`topicmodels`](https://cran.r-project.org/web/packages/topicmodels/index.html)
- `quanteda`: Quantitative Analysis of Textual Data - http://quanteda.io/

- Still need to know these because `topicmodels` take `tm` objects
- `tidytext` has a number of `cast_*` functions


## Example classification

```{r AP}
library(tm); 
data("AssociatedPress", package = "topicmodels"); AssociatedPress
class(AssociatedPress); head(tidy(AssociatedPress)) # generics::tidy
```


## Compare frequencies: Jane Austen vs. the AP

```{r}
comparison = tidy(AssociatedPress) %>%
  group_by(term) %>% 
  summarise(AP = sum(count)) %>% # add up the counts
  rename(word = term) %>% 
  inner_join(count(tidy_books, word, name = "Austen")) %>%
  mutate(AP = AP / sum(AP),
         Austen = Austen / sum(Austen),
         diff = AP - Austen) %>% 
  arrange(desc(abs(diff)))
head(comparison)
```

## Bag of words: Count of words by Document


```{r}
tidy_freq = tidy_books %>% 
  dplyr::ungroup() %>% 
  count(book, word, name = "count")
head(tidy_freq)
```

## Bag of words

`nonum` removes any words that are all numeric (many ways of doing this):

```{r}
nonum = tidy_freq %>% 
  filter(is.na(as.numeric(word)))
head(nonum)
```


## Combine "bags"

```{r}
tidy_ap = tidy(AssociatedPress) %>% 
  rename(book = document, 
         word = term, 
         count = count)
dat = rbind(tidy_ap, tidy_freq) 
head(dat)
```


## Term-document matrices

Make a `DocuemntTermMatrix`/reshape the data (`tm` object):

```{r}
dtm = dat %>% cast_dtm(document = book, term = word, value = count)
inspect(dtm[1:6,1:10])
```

## Normalized Matrices

Here we normalize documents based on number of words:

```{r normalize}
dtm = as.matrix(dtm)
dtm = dtm/rowSums(dtm)
```

## Classify

Show the similarity (based on count correlations with first document):
```{r, fig.width = 8, fig.height = 5}
cor1 = cor(dtm[1,], t(dtm))[1,]; print(cor1[1:5]);
```

```{r, fig.width = 8, fig.height = 5, echo = FALSE}
plot(cor1)
```

## Classify

We see that there is a large clustering of Austen compared to AP:
```{r Sense_Sensibility, fig.width = 8, fig.height = 5}
cor_ss = cor(dtm["Sense & Sensibility",], t(dtm))[1,]; print(cor_ss[1:5]); 
```

```{r Sense_Sensibility_plot, fig.width = 8, fig.height = 5, echo = FALSE}
plot(cor_ss)
```


## Classify

The max similarity is not symmetrical (closest document/book to document 1 does not have document 1 as its closest document/book):

```{r}
(index = which.max(cor1[-1]))
cor_ss = cor(dtm[index,],t(dtm))[1,]
which.max(cor_ss[-index]) # not 1!
```



## Unsupervised topics

To see if we can separate the words into different topics, we can use an Latent Dirichlet Allocation (LDA).  

Assumptions: 

1. Every document is a mixture of topics. 
2. Every topic is a mixture of words. 

The LDA estimates how much:

- Each topic contributes to each document
- Each word contributes to each topic 

## Unsupervised topics

For `topicmodels::LDA`, we need a `DocumentTermMatrix` (DTM) and can create one using `tidytext::cast_dtm`:

```{r}
dtm = tidy_books %>% 
  count(book, word) %>% 
  tidytext::cast_dtm(document = book, term = word, value = n)
unique_indexes = unique(dtm$i) # get the index of each unique value

# let's try 6 topics
lda = topicmodels::LDA(dtm, k = 6L, control = list(seed = 20230910))
topics = tidy(lda, matrix = "beta")
head(topics)
```

## Unsupervised topics

```{r tidymod}
# get the to2 12 terms for each topic
top_terms = topics  %>%
  group_by(topic) %>% 
  top_n(12, beta) %>% # get the top 12 beta by topic
  ungroup() %>% # ungroup
  arrange(topic, -beta) # arrange words in descending informativeness

top_terms %>% # take the top terms
  mutate(term = reorder(term, beta)) %>% # sort terms by beta value 
  ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
  geom_col(show.legend = FALSE) + # as a bar plot
  facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
  labs(x = NULL, y = "Beta") + # no x label, change y label 
  coord_flip() 
```

## Two Topics

Here we just do 2 topics and show the top differentiated terms:
```{r}
# let's try 2 topics
lda = topicmodels::LDA(dtm, k = 2L, control = list(seed = 20230910))
topics = tidy(lda, matrix = "beta")
beta_wide = topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))
```

```{r plot_ratio, echo = FALSE}
b = beta_wide %>% 
  mutate(sign = sign(log_ratio)) %>% 
  arrange(desc(abs(log_ratio))) %>% 
  group_by(sign) %>% 
  slice(1:10L) %>% 
  ungroup() %>% 
  arrange(log_ratio)
b = b %>% 
  mutate(term = factor(term, levels = term))
b %>% 
  ggplot(aes(x = term, y = log_ratio)) + geom_bar(stat = "identity") +
  coord_flip() + xlab(NULL) + 
  ylab("Log Ratio of Beta between topic 2 and topic 1")
```
## Topics contributing to Documents

We can extract the per-document-per-topic probabilities with the gamma param:
```{r}
(docs = tidy(lda, matrix = "gamma") %>% 
  arrange(topic, desc(gamma)))
```


## AP and Jane Austen

What happens when we put the AP and Jane Austen data together?

```{r}
dtm = dat %>% cast_dtm(document = book, term = word, value = count)
lda = topicmodels::LDA(dtm, k = 2L, control = list(seed = 20230910))
topics = tidy(lda, matrix = "beta")
docs = tidy(lda, matrix = "gamma") %>% 
  arrange(topic, desc(gamma))
docs %>% filter(grepl("[a-z]", tolower(document)))
```

## AP and Jane Austen

```{r plot_ratio_both, echo = FALSE}
beta_wide = topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))
b = beta_wide %>% 
  mutate(sign = sign(log_ratio)) %>% 
  arrange(desc(abs(log_ratio))) %>% 
  group_by(sign) %>% 
  slice(1:10L) %>% 
  ungroup() %>% 
  arrange(log_ratio)
b = b %>% 
  mutate(term = factor(term, levels = term))
b %>% 
  ggplot(aes(x = term, y = log_ratio)) + geom_bar(stat = "identity") +
  coord_flip() + xlab(NULL) + 
  ylab("Log Ratio of Beta between topic 2 and topic 1")
```

## Other Models

Latent Semantic Analysis (LSA), essentially SVD on the document matrix

```{r}
library("quanteda.textmodels")
full_dfm = dat %>% cast_dfm(document = book, term = word, value = count)
lsa <- textmodel_lsa(full_dfm)
head(lsa$docs)# the number of dimensions to be included in output
head(lsa$features)
```


# https://www.tidytextmining.com/
